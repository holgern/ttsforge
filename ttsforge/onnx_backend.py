"""ONNX backend for ttsforge - native ONNX TTS without external dependencies."""

import asyncio
import io
import os
import re
import sqlite3
from collections.abc import AsyncGenerator, Generator
from dataclasses import dataclass
from pathlib import Path
from typing import TYPE_CHECKING, Any, Callable, Literal, Optional
from urllib.request import urlretrieve

import numpy as np
import onnxruntime as rt

from .tokenizer import EspeakConfig, Tokenizer
from .trim import trim as trim_audio
from .utils import get_user_cache_path

if TYPE_CHECKING:
    from .phonemes import PhonemeSegment

# Maximum phoneme length for a single inference
MAX_PHONEME_LENGTH = 510

# Sample rate for Kokoro models
SAMPLE_RATE = 24000

# Model quality type
ModelQuality = Literal[
    "fp32", "fp16", "q8", "q8f16", "q4", "q4f16", "uint8", "uint8f16"
]
DEFAULT_MODEL_QUALITY: ModelQuality = "fp32"

# Quality to filename mapping
MODEL_QUALITY_FILES: dict[str, str] = {
    "fp32": "model.onnx",
    "fp16": "model_fp16.onnx",
    "q8": "model_quantized.onnx",
    "q8f16": "model_q8f16.onnx",
    "q4": "model_q4.onnx",
    "q4f16": "model_q4f16.onnx",
    "uint8": "model_uint8.onnx",
    "uint8f16": "model_uint8f16.onnx",
}

# URLs for model files (Hugging Face)
MODEL_BASE_URL = (
    "https://huggingface.co/onnx-community/Kokoro-82M-v1.0-ONNX/resolve/main/onnx"
)
VOICES_BASE_URL = (
    "https://huggingface.co/onnx-community/Kokoro-82M-v1.0-ONNX/resolve/main/voices"
)
CONFIG_URL = "https://huggingface.co/hexgrad/Kokoro-82M/resolve/main/config.json"

# All available voice names
VOICE_NAMES = [
    "af",
    "af_alloy",
    "af_aoede",
    "af_bella",
    "af_heart",
    "af_jessica",
    "af_kore",
    "af_nicole",
    "af_nova",
    "af_river",
    "af_sarah",
    "af_sky",
    "am_adam",
    "am_echo",
    "am_eric",
    "am_fenrir",
    "am_liam",
    "am_michael",
    "am_onyx",
    "am_puck",
    "am_santa",
    "bf_alice",
    "bf_emma",
    "bf_isabella",
    "bf_lily",
    "bm_daniel",
    "bm_fable",
    "bm_george",
    "bm_lewis",
    "ef_dora",
    "em_alex",
    "em_santa",
    "ff_siwis",
    "hf_alpha",
    "hf_beta",
    "hm_omega",
    "hm_psi",
    "if_sara",
    "im_nicola",
    "jf_alpha",
    "jf_gongitsune",
    "jf_nezumi",
    "jf_tebukuro",
    "jm_kumo",
    "pf_dora",
    "pm_alex",
    "pm_santa",
    "zf_xiaobei",
    "zf_xiaoni",
    "zf_xiaoxiao",
    "zm_yunjian",
    "zm_yunxi",
    "zm_yunxia",
    "zm_yunyang",
]


@dataclass
class VoiceBlend:
    """Represents a blend of multiple voices."""

    voices: list[tuple[str, float]]  # List of (voice_name, weight) tuples

    @classmethod
    def parse(cls, blend_str: str) -> "VoiceBlend":
        """
        Parse a voice blend string.

        Format: "voice1:weight1,voice2:weight2" or "voice1:50,voice2:50"
        Weights should sum to 100 (percentages).

        Example: "af_nicole:50,am_michael:50"
        """
        voices = []
        for part in blend_str.split(","):
            part = part.strip()
            if ":" in part:
                voice_name, weight_str = part.split(":", 1)
                weight = float(weight_str) / 100.0  # Convert percentage to fraction
            else:
                voice_name = part
                weight = 1.0
            voices.append((voice_name.strip(), weight))

        # Normalize weights if they don't sum to 1
        total_weight = sum(w for _, w in voices)
        if total_weight > 0 and abs(total_weight - 1.0) > 0.01:
            voices = [(v, w / total_weight) for v, w in voices]

        return cls(voices=voices)


# =============================================================================
# Path helper functions
# =============================================================================


def get_model_dir() -> Path:
    """Get the directory for storing ONNX model files."""
    return get_user_cache_path("models")


def get_voices_dir() -> Path:
    """Get the directory for storing voice files."""
    return get_user_cache_path("voices")


def get_config_path() -> Path:
    """Get the path to the cached config.json."""
    return get_user_cache_path() / "config.json"


def get_voices_bin_path() -> Path:
    """Get the path to the combined voices.bin.npz file."""
    return get_user_cache_path() / "voices.bin.npz"


def get_model_filename(quality: ModelQuality = DEFAULT_MODEL_QUALITY) -> str:
    """Get the model filename for a quality level."""
    return MODEL_QUALITY_FILES[quality]


def get_model_path(quality: ModelQuality = DEFAULT_MODEL_QUALITY) -> Path:
    """Get the full path to a model file for a given quality."""
    filename = get_model_filename(quality)
    return get_model_dir() / filename


def get_voice_path(voice_name: str) -> Path:
    """Get the full path to an individual voice file."""
    return get_voices_dir() / f"{voice_name}.bin"


# =============================================================================
# Download check functions
# =============================================================================


def is_config_downloaded() -> bool:
    """Check if config.json is downloaded."""
    config_path = get_config_path()
    return config_path.exists() and config_path.stat().st_size > 0


def is_model_downloaded(quality: ModelQuality = DEFAULT_MODEL_QUALITY) -> bool:
    """Check if a model file is already downloaded for a given quality."""
    model_path = get_model_path(quality)
    return model_path.exists() and model_path.stat().st_size > 0


def is_voice_downloaded(voice_name: str) -> bool:
    """Check if an individual voice file is already downloaded."""
    voice_path = get_voice_path(voice_name)
    return voice_path.exists() and voice_path.stat().st_size > 0


def are_voices_downloaded() -> bool:
    """Check if the combined voices.bin file exists."""
    voices_bin_path = get_voices_bin_path()
    return voices_bin_path.exists() and voices_bin_path.stat().st_size > 0


def are_models_downloaded(quality: ModelQuality = DEFAULT_MODEL_QUALITY) -> bool:
    """Check if model, config, and voices.bin are downloaded."""
    return (
        is_config_downloaded()
        and is_model_downloaded(quality)
        and are_voices_downloaded()
    )


# =============================================================================
# Download functions
# =============================================================================


def _download_file(
    url: str,
    dest_path: Path,
    progress_callback: Optional[Callable[[int, int], None]] = None,
    force: bool = False,
) -> Path:
    """
    Download a file from URL to destination path.

    Args:
        url: URL to download from
        dest_path: Destination path for the file
        progress_callback: Optional callback for progress updates (current, total)
        force: Force re-download even if file exists

    Returns:
        Path to the downloaded file
    """
    if dest_path.exists() and not force:
        return dest_path

    dest_path.parent.mkdir(parents=True, exist_ok=True)

    # Download with progress
    temp_path = dest_path.with_suffix(".tmp")

    def _report_progress(block_num: int, block_size: int, total_size: int) -> None:
        if progress_callback and total_size > 0:
            downloaded = block_num * block_size
            progress_callback(min(downloaded, total_size), total_size)

    try:
        urlretrieve(url, temp_path, reporthook=_report_progress)
        temp_path.rename(dest_path)
    except Exception:
        temp_path.unlink(missing_ok=True)
        raise

    return dest_path


def download_config(
    progress_callback: Optional[Callable[[int, int], None]] = None,
    force: bool = False,
) -> Path:
    """
    Download config.json from Hugging Face.

    Args:
        progress_callback: Optional callback for progress updates (current, total)
        force: Force re-download even if file exists

    Returns:
        Path to the downloaded config file
    """
    return _download_file(CONFIG_URL, get_config_path(), progress_callback, force)


def download_model(
    quality: ModelQuality = DEFAULT_MODEL_QUALITY,
    progress_callback: Optional[Callable[[int, int], None]] = None,
    force: bool = False,
) -> Path:
    """
    Download a model file for the specified quality.

    Args:
        quality: Model quality/quantization level
        progress_callback: Optional callback for progress updates (current, total)
        force: Force re-download even if file exists

    Returns:
        Path to the downloaded model file
    """
    filename = get_model_filename(quality)
    url = f"{MODEL_BASE_URL}/{filename}"
    return _download_file(url, get_model_path(quality), progress_callback, force)


def download_voice(
    voice_name: str,
    progress_callback: Optional[Callable[[int, int], None]] = None,
    force: bool = False,
) -> Path:
    """
    Download a single voice file.

    Args:
        voice_name: Name of the voice to download
        progress_callback: Optional callback for progress updates (current, total)
        force: Force re-download even if file exists

    Returns:
        Path to the downloaded voice file
    """
    url = f"{VOICES_BASE_URL}/{voice_name}.bin"
    return _download_file(url, get_voice_path(voice_name), progress_callback, force)


def download_all_voices(
    progress_callback: Optional[Callable[[str, int, int], None]] = None,
    force: bool = False,
) -> Path:
    """
    Download all voice files and config, then combine into a single voices.bin file.

    Downloads individual voice .bin files and config.json from Hugging Face,
    loads them, and saves them as a combined numpy archive (voices.bin) for
    efficient loading.

    Args:
        progress_callback: Optional callback (voice_name, current_index, total_count)
        force: Force re-download even if files exist

    Returns:
        Path to the combined voices.bin file
    """
    voices_bin_path = get_voices_bin_path()

    # If voices.bin already exists and not forcing, skip download
    if voices_bin_path.exists() and not force:
        return voices_bin_path

    voices_dir = get_voices_dir()
    voices_dir.mkdir(parents=True, exist_ok=True)

    # Download config.json first
    if progress_callback:
        progress_callback("config.json", 0, len(VOICE_NAMES) + 1)
    download_config(force=force)

    total = len(VOICE_NAMES)
    voices: dict[str, np.ndarray] = {}

    for idx, voice_name in enumerate(VOICE_NAMES):
        if progress_callback:
            progress_callback(voice_name, idx + 1, total)

        # Download individual voice file
        voice_path = download_voice(voice_name, force=force)

        # Load the voice data from .bin file
        voice_data = np.fromfile(voice_path, dtype=np.float32).reshape(-1, 1, 256)
        voices[voice_name] = voice_data

    # Save all voices to a single .bin file using np.savez
    voices_bin_path.parent.mkdir(parents=True, exist_ok=True)
    np.savez(str(voices_bin_path), **voices)  # type: ignore[arg-type]

    return voices_bin_path


def download_all_models(
    quality: ModelQuality = DEFAULT_MODEL_QUALITY,
    progress_callback: Optional[Callable[[str, int, int], None]] = None,
    force: bool = False,
) -> dict[str, Path]:
    """
    Download config, model, and all voice files.

    Args:
        quality: Model quality/quantization level
        progress_callback: Optional callback (filename, current, total)
        force: Force re-download even if files exist

    Returns:
        Dict mapping filename to path
    """
    paths: dict[str, Path] = {}

    # Download config
    if progress_callback:
        progress_callback("config.json", 0, 0)
    paths["config.json"] = download_config(force=force)

    # Download model
    model_filename = get_model_filename(quality)
    if progress_callback:
        progress_callback(model_filename, 0, 0)
    paths[model_filename] = download_model(quality, force=force)

    # Download all voices and combine into voices.bin
    paths["voices.bin"] = download_all_voices(progress_callback, force)

    return paths


class KokoroONNX:
    """
    Native ONNX backend for TTS generation.

    This class provides direct ONNX inference without external dependencies.
    Includes embedded tokenizer for phoneme/token-based generation.
    """

    def __init__(
        self,
        model_path: Optional[Path] = None,
        voices_path: Optional[Path] = None,
        use_gpu: bool = False,
        vocab_version: str = "v1.0",
        espeak_config: Optional[EspeakConfig] = None,
        model_quality: Optional[ModelQuality] = None,
    ) -> None:
        """
        Initialize the Kokoro ONNX backend.

        Args:
            model_path: Path to the ONNX model file (auto-downloaded if None)
            voices_path: Path to the voices.bin file (auto-downloaded if None)
            use_gpu: Whether to use GPU acceleration (requires onnxruntime-gpu)
            vocab_version: Vocabulary version for tokenizer
            espeak_config: Optional espeak-ng configuration
            model_quality: Model quality/quantization level (default from config)
        """
        self._session: Optional[rt.InferenceSession] = None
        self._voices_data: Optional[dict[str, np.ndarray]] = None
        self._np = np
        self._use_gpu = use_gpu

        # Resolve model quality from config if not specified
        resolved_quality: ModelQuality = DEFAULT_MODEL_QUALITY
        if model_quality is not None:
            resolved_quality = model_quality
        else:
            from .utils import load_config

            cfg = load_config()
            quality_from_cfg = cfg.get("model_quality", DEFAULT_MODEL_QUALITY)
            # Validate it's a valid quality option and cast to ModelQuality
            if quality_from_cfg in MODEL_QUALITY_FILES:
                resolved_quality = str(quality_from_cfg)  # type: ignore[assignment]
        self._model_quality: ModelQuality = resolved_quality

        # Resolve paths
        if model_path is None:
            model_path = get_model_path(self._model_quality)
        if voices_path is None:
            voices_path = get_voices_bin_path()

        self._model_path = model_path
        self._voices_path = voices_path

        # Voice database connection (for kokovoicelab integration)
        self._voice_db: Optional[sqlite3.Connection] = None

        # Tokenizer for phoneme-based generation
        self._tokenizer: Optional[Tokenizer] = None
        self._vocab_version = vocab_version
        self._espeak_config = espeak_config

    @property
    def tokenizer(self) -> Tokenizer:
        """Get the tokenizer instance (lazily initialized)."""
        if self._tokenizer is None:
            self._tokenizer = Tokenizer(
                espeak_config=self._espeak_config,
                vocab_version=self._vocab_version,
            )
        return self._tokenizer

    def _ensure_models(self) -> None:
        """Ensure model and voice files are downloaded."""
        if not self._model_path.exists():
            download_model(self._model_quality)
        if not self._voices_path.exists():
            download_all_voices()
        if not is_config_downloaded():
            download_config()

    def _init_kokoro(self) -> None:
        """Initialize the ONNX session and load voices."""
        if self._session is not None:
            return

        self._ensure_models()

        # Set up execution providers based on GPU preference and environment
        providers = ["CPUExecutionProvider"]
        if self._use_gpu:
            available = rt.get_available_providers()
            # Prefer CUDA, then CoreML, then DirectML
            for provider in [
                "CUDAExecutionProvider",
                "CoreMLExecutionProvider",
                "DmlExecutionProvider",
            ]:
                if provider in available:
                    providers = [provider, "CPUExecutionProvider"]
                    break

        # Allow environment override
        env_provider = os.getenv("ONNX_PROVIDER")
        if env_provider:
            providers = [env_provider]

        # Load ONNX model
        self._session = rt.InferenceSession(str(self._model_path), providers=providers)

        # Load voices (numpy archive with voice style vectors)
        self._voices_data = dict(np.load(str(self._voices_path), allow_pickle=True))

    def _create_audio_internal(
        self, phonemes: str, voice: np.ndarray, speed: float, new_format: bool = True
    ) -> tuple[np.ndarray, int]:
        """
        Core ONNX inference for a single phoneme batch.

        Args:
            phonemes: Phoneme string (will be truncated if > MAX_PHONEME_LENGTH)
            voice: Voice style vector
            speed: Speech speed multiplier

        Returns:
            Tuple of (audio samples, sample rate)
        """
        assert self._session is not None

        # Truncate phonemes if too long
        phonemes = phonemes[:MAX_PHONEME_LENGTH]
        tokens = self.tokenizer.tokenize(phonemes)

        # Get voice style for this token length
        voice_style = voice[len(tokens)]

        # Pad tokens with start/end tokens
        tokens_padded = [[0, *tokens, 0]]

        # Check input names to determine model version
        input_names = [i.name for i in self._session.get_inputs()]
        if "input_ids" in input_names and not new_format:
            # Newer model format (exported with input_ids, expects int32 speed)
            # Speed is typically 1 for normal speed, convert float to int
            speed_int = max(1, int(round(speed)))
            inputs = {
                "input_ids": np.array(tokens_padded, dtype=np.int64),
                "style": np.array(voice_style, dtype=np.float32),
                "speed": np.array([speed_int], dtype=np.int32),
            }
        elif "input_ids" in input_names and new_format:
            # Original model format (kokoro-onnx release model, uses float speed)
            inputs = {
                "input_ids": tokens_padded,
                "style": voice_style,
                "speed": np.ones(1, dtype=np.float32) * speed,
            }

        else:
            # Original model format (kokoro-onnx release model, uses float speed)
            inputs = {
                "tokens": tokens_padded,
                "style": voice_style,
                "speed": np.ones(1, dtype=np.float32) * speed,
            }

        result = self._session.run(None, inputs)[0]
        if new_format:
            audio: np.ndarray = np.asarray(result).T
        else:
            audio: np.ndarray = np.asarray(result)
        return audio, SAMPLE_RATE

    def _split_phonemes(self, phonemes: str) -> list[str]:
        """
        Split phonemes into batches at punctuation marks.

        Args:
            phonemes: Full phoneme string

        Returns:
            List of phoneme batches, each <= MAX_PHONEME_LENGTH
        """
        # Split on punctuation marks while keeping them
        words = re.split(r"([.,!?;])", phonemes)
        batches = []
        current = ""

        for part in words:
            part = part.strip()
            if not part:
                continue
            if len(current) + len(part) + 1 >= MAX_PHONEME_LENGTH:
                if current:
                    batches.append(current.strip())
                current = part
            elif part in ".,!?;":
                current += part
            else:
                if current:
                    current += " "
                current += part

        if current:
            batches.append(current.strip())
        return batches

    def get_voices(self) -> list[str]:
        """Get list of available voice names."""
        self._init_kokoro()
        assert self._voices_data is not None
        return list(sorted(self._voices_data.keys()))

    def get_voice_style(self, voice_name: str) -> np.ndarray:
        """
        Get the style vector for a voice.

        Args:
            voice_name: Name of the voice

        Returns:
            Numpy array representing the voice style
        """
        self._init_kokoro()
        assert self._voices_data is not None
        return self._voices_data[voice_name]

    def create_blended_voice(self, blend: VoiceBlend) -> np.ndarray:
        """
        Create a blended voice from multiple voices.

        Args:
            blend: VoiceBlend object specifying voices and weights

        Returns:
            Numpy array representing the blended voice style
        """
        self._init_kokoro()

        if len(blend.voices) == 1:
            voice_name, _ = blend.voices[0]
            return self.get_voice_style(voice_name)

        # Get style vectors and blend them
        blended: Optional[np.ndarray] = None
        for voice_name, weight in blend.voices:
            style = self.get_voice_style(voice_name)
            weighted = style * weight
            if blended is None:
                blended = weighted
            else:
                blended = np.add(blended, weighted)

        # This should never be None if blend.voices is not empty
        assert blended is not None, "No voices in blend"
        return blended

    def create(
        self,
        text: str,
        voice: str | np.ndarray | VoiceBlend,
        speed: float = 1.0,
        lang: str = "en-us",
    ) -> tuple[np.ndarray, int]:
        """
        Generate audio from text.

        Args:
            text: Text to synthesize
            voice: Voice name, style vector, or VoiceBlend
            speed: Speech speed (1.0 = normal)
            lang: Language code (e.g., 'en-us', 'en-gb', 'es', 'fr')

        Returns:
            Tuple of (audio samples as numpy array, sample rate)
        """
        self._init_kokoro()

        # Resolve voice to style vector
        if isinstance(voice, VoiceBlend):
            voice_style = self.create_blended_voice(voice)
        elif isinstance(voice, str):
            voice_style = self.get_voice_style(voice)
        else:
            voice_style = voice

        # Convert text to phonemes
        phonemes = self.tokenizer.phonemize(text, lang=lang)

        # Split phonemes into batches and generate audio
        batches = self._split_phonemes(phonemes)
        audio_parts = []

        for batch in batches:
            audio_part, _ = self._create_audio_internal(batch, voice_style, speed)
            # Trim silence from each part
            audio_part, _ = trim_audio(audio_part)
            audio_parts.append(audio_part)

        if not audio_parts:
            return np.array([], dtype=np.float32), SAMPLE_RATE

        return np.concatenate(audio_parts), SAMPLE_RATE

    def create_from_phonemes(
        self,
        phonemes: str,
        voice: str | np.ndarray | VoiceBlend,
        speed: float = 1.0,
    ) -> tuple[np.ndarray, int]:
        """
        Generate audio from phonemes directly.

        This bypasses text-to-phoneme conversion, useful when working
        with pre-tokenized content from PhonemeBook.

        Args:
            phonemes: IPA phoneme string
            voice: Voice name, style vector, or VoiceBlend
            speed: Speech speed (1.0 = normal)

        Returns:
            Tuple of (audio samples as numpy array, sample rate)
        """
        self._init_kokoro()

        # Resolve voice to style vector if needed
        if isinstance(voice, VoiceBlend):
            voice_style = self.create_blended_voice(voice)
        elif isinstance(voice, str):
            voice_style = self.get_voice_style(voice)
        else:
            voice_style = voice

        # kokoro-onnx supports direct phoneme input via create method with ps parameter
        # But we need to convert to tokens first
        tokens = self.tokenizer.tokenize(phonemes)
        return self.create_from_tokens(tokens, voice_style, speed)

    def create_from_tokens(
        self,
        tokens: list[int],
        voice: str | np.ndarray | VoiceBlend,
        speed: float = 1.0,
    ) -> tuple[np.ndarray, int]:
        """
        Generate audio from token IDs directly.

        This provides the lowest-level interface, useful for pre-tokenized
        content and maximum control.

        Args:
            tokens: List of token IDs
            voice: Voice name, style vector, or VoiceBlend
            speed: Speech speed (1.0 = normal)

        Returns:
            Tuple of (audio samples as numpy array, sample rate)
        """
        self._init_kokoro()

        # Resolve voice to style vector
        if isinstance(voice, VoiceBlend):
            voice_style = self.create_blended_voice(voice)
        elif isinstance(voice, str):
            voice_style = self.get_voice_style(voice)
        else:
            voice_style = voice

        # Detokenize to phonemes and generate audio
        phonemes = self.tokenizer.detokenize(tokens)

        # Split phonemes into batches and generate audio
        batches = self._split_phonemes(phonemes)
        audio_parts = []

        for batch in batches:
            audio_part, _ = self._create_audio_internal(batch, voice_style, speed)
            # Trim silence from each part
            audio_part, _ = trim_audio(audio_part)
            audio_parts.append(audio_part)

        if not audio_parts:
            return np.array([], dtype=np.float32), SAMPLE_RATE

        return np.concatenate(audio_parts), SAMPLE_RATE

    def create_from_segment(
        self,
        segment: "PhonemeSegment",
        voice: str | np.ndarray | VoiceBlend,
        speed: float = 1.0,
    ) -> tuple[np.ndarray, int]:
        """
        Generate audio from a PhonemeSegment.

        Args:
            segment: PhonemeSegment with phonemes and tokens
            voice: Voice name, style vector, or VoiceBlend
            speed: Speech speed (1.0 = normal)

        Returns:
            Tuple of (audio samples as numpy array, sample rate)
        """
        # Use tokens if available, otherwise use phonemes
        if segment.tokens:
            return self.create_from_tokens(segment.tokens, voice, speed)
        elif segment.phonemes:
            return self.create_from_phonemes(segment.phonemes, voice, speed)
        else:
            # Fall back to text
            return self.create(segment.text, voice, speed, segment.lang)

    def phonemize(self, text: str, lang: str = "en-us") -> str:
        """
        Convert text to phonemes.

        Args:
            text: Input text
            lang: Language code

        Returns:
            Phoneme string
        """
        return self.tokenizer.phonemize(text, lang=lang)

    def tokenize(self, phonemes: str) -> list[int]:
        """
        Convert phonemes to tokens.

        Args:
            phonemes: Phoneme string

        Returns:
            List of token IDs
        """
        return self.tokenizer.tokenize(phonemes)

    def detokenize(self, tokens: list[int]) -> str:
        """
        Convert tokens back to phonemes.

        Args:
            tokens: List of token IDs

        Returns:
            Phoneme string
        """
        return self.tokenizer.detokenize(tokens)

    def text_to_tokens(self, text: str, lang: str = "en-us") -> list[int]:
        """
        Convert text directly to tokens.

        Args:
            text: Input text
            lang: Language code

        Returns:
            List of token IDs
        """
        return self.tokenizer.text_to_tokens(text, lang=lang)

    def generate_chunks(
        self,
        text: str,
        voice: str | np.ndarray | VoiceBlend,
        speed: float = 1.0,
        lang: str = "en-us",
        chunk_size: int = 500,
    ):
        """
        Generate audio in chunks for long text.

        This splits text into manageable chunks and yields audio for each.
        Useful for progress tracking during long conversions.

        Args:
            text: Text to synthesize
            voice: Voice name, style vector, or VoiceBlend
            speed: Speech speed
            lang: Language code
            chunk_size: Approximate character count per chunk

        Yields:
            Tuple of (audio samples, sample rate, text chunk)
        """
        self._init_kokoro()

        # Resolve voice once
        if isinstance(voice, VoiceBlend):
            voice_style = self.create_blended_voice(voice)
        elif isinstance(voice, str):
            voice_style = self.get_voice_style(voice)
        else:
            voice_style = voice

        # Split text into chunks at sentence boundaries
        chunks = self._split_text(text, chunk_size)

        for chunk in chunks:
            if not chunk.strip():
                continue

            # Convert chunk to phonemes and generate audio
            phonemes = self.tokenizer.phonemize(chunk, lang=lang)
            batches = self._split_phonemes(phonemes)
            audio_parts = []

            for batch in batches:
                audio_part, _ = self._create_audio_internal(batch, voice_style, speed)
                audio_part, _ = trim_audio(audio_part)
                audio_parts.append(audio_part)

            if audio_parts:
                samples = np.concatenate(audio_parts)
                yield samples, SAMPLE_RATE, chunk

    def _split_text(self, text: str, chunk_size: int) -> list[str]:
        """
        Split text into chunks at sentence boundaries.

        Args:
            text: Text to split
            chunk_size: Target chunk size in characters

        Returns:
            List of text chunks
        """
        # Split on sentence boundaries while keeping the delimiter
        sentences = re.split(r"(?<=[.!?])\s+", text)

        chunks = []
        current_chunk = ""

        for sentence in sentences:
            if len(current_chunk) + len(sentence) <= chunk_size:
                current_chunk += sentence + " "
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence + " "

        if current_chunk:
            chunks.append(current_chunk.strip())

        return chunks

    # Voice Database Integration (from kokovoicelab)

    def load_voice_database(self, db_path: Path) -> None:
        """
        Load a voice database for custom/synthetic voices.

        Args:
            db_path: Path to the SQLite voice database
        """
        if self._voice_db is not None:
            self._voice_db.close()

        # Register numpy array converter
        sqlite3.register_converter("array", self._convert_array)
        self._voice_db = sqlite3.connect(
            str(db_path), detect_types=sqlite3.PARSE_DECLTYPES
        )

    def _convert_array(self, blob: bytes) -> np.ndarray:
        """Convert binary blob back to numpy array."""
        out = io.BytesIO(blob)
        return np.load(out)

    def get_voice_from_database(self, voice_name: str) -> Optional[np.ndarray]:
        """
        Get a voice style vector from the database.

        Args:
            voice_name: Name of the voice in the database

        Returns:
            Voice style vector or None if not found
        """
        if self._voice_db is None:
            return None

        cursor = self._voice_db.cursor()
        cursor.execute(
            "SELECT style_vector FROM voices WHERE name = ?",
            (voice_name,),
        )
        row = cursor.fetchone()

        if row:
            return row[0]
        return None

    def list_database_voices(self) -> list[dict[str, Any]]:
        """
        List all voices in the database.

        Returns:
            List of voice metadata dictionaries
        """
        if self._voice_db is None:
            return []

        cursor = self._voice_db.cursor()
        cursor.execute(
            """
            SELECT name, gender, language, quality, is_synthetic, notes
            FROM voices
            ORDER BY quality DESC
            """
        )

        voices = []
        for row in cursor.fetchall():
            voices.append(
                {
                    "name": row[0],
                    "gender": row[1],
                    "language": row[2],
                    "quality": row[3],
                    "is_synthetic": bool(row[4]),
                    "notes": row[5],
                }
            )

        return voices

    def interpolate_voices(
        self,
        voice1: str | np.ndarray,
        voice2: str | np.ndarray,
        factor: float = 0.5,
    ) -> np.ndarray:
        """
        Interpolate between two voices.

        This uses the interpolation method from kokovoicelab to create
        voices that lie on the line between two source voices.

        Args:
            voice1: First voice (name or style vector)
            voice2: Second voice (name or style vector)
            factor: Interpolation factor (0.0 = voice1, 1.0 = voice2)

        Returns:
            Interpolated voice style vector
        """
        self._init_kokoro()

        # Resolve to style vectors
        if isinstance(voice1, str):
            style1 = self.get_voice_style(voice1)
        else:
            style1 = voice1

        if isinstance(voice2, str):
            style2 = self.get_voice_style(voice2)
        else:
            style2 = voice2

        # Use kokovoicelab's interpolation method
        diff_vector = style2 - style1
        midpoint = (style1 + style2) / 2
        return midpoint + (diff_vector * factor / 2)

    async def create_stream(
        self,
        text: str,
        voice: str | np.ndarray | VoiceBlend,
        speed: float = 1.0,
        lang: str = "en-us",
    ) -> AsyncGenerator[tuple[np.ndarray, int, str], None]:
        """
        Stream audio creation asynchronously, yielding chunks as they are processed.

        This method generates audio in the background and yields chunks as soon as
        they're ready, enabling real-time playback while generation continues.

        Args:
            text: Text to synthesize
            voice: Voice name, style vector, or VoiceBlend
            speed: Speech speed (1.0 = normal)
            lang: Language code (e.g., 'en-us', 'en-gb')

        Yields:
            Tuple of (audio samples as numpy array, sample rate, text chunk)
        """
        self._init_kokoro()

        # Resolve voice to style vector
        if isinstance(voice, VoiceBlend):
            voice_style = self.create_blended_voice(voice)
        elif isinstance(voice, str):
            voice_style = self.get_voice_style(voice)
        else:
            voice_style = voice

        # Convert text to phonemes
        phonemes = self.tokenizer.phonemize(text, lang=lang)

        # Split phonemes into batches
        batched_phonemes = self._split_phonemes(phonemes)

        # Create a queue for passing audio chunks
        queue: asyncio.Queue[tuple[np.ndarray, int, str] | None] = asyncio.Queue()

        async def process_batches() -> None:
            """Process phoneme batches in the background."""
            loop = asyncio.get_event_loop()
            for phoneme_batch in batched_phonemes:
                # Execute blocking ONNX inference in a thread executor
                audio_part, sample_rate = await loop.run_in_executor(
                    None, self._create_audio_internal, phoneme_batch, voice_style, speed
                )
                # Trim silence
                audio_part, _ = trim_audio(audio_part)
                await queue.put((audio_part, sample_rate, phoneme_batch))
            await queue.put(None)  # Signal end of stream

        # Start processing in the background
        asyncio.create_task(process_batches())

        # Yield chunks as they become available
        while True:
            chunk = await queue.get()
            if chunk is None:
                break
            yield chunk

    def create_stream_sync(
        self,
        text: str,
        voice: str | np.ndarray | VoiceBlend,
        speed: float = 1.0,
        lang: str = "en-us",
    ) -> Generator[tuple[np.ndarray, int, str], None, None]:
        """
        Stream audio creation synchronously, yielding chunks as they are processed.

        This is a synchronous version of create_stream for use in non-async contexts.
        It yields audio chunks immediately as they're generated.

        Args:
            text: Text to synthesize
            voice: Voice name, style vector, or VoiceBlend
            speed: Speech speed (1.0 = normal)
            lang: Language code (e.g., 'en-us', 'en-gb')

        Yields:
            Tuple of (audio samples as numpy array, sample rate, text chunk)
        """
        self._init_kokoro()

        # Resolve voice to style vector
        if isinstance(voice, VoiceBlend):
            voice_style = self.create_blended_voice(voice)
        elif isinstance(voice, str):
            voice_style = self.get_voice_style(voice)
        else:
            voice_style = voice

        # Convert text to phonemes
        phonemes = self.tokenizer.phonemize(text, lang=lang)

        # Split phonemes into batches
        batched_phonemes = self._split_phonemes(phonemes)

        for phoneme_batch in batched_phonemes:
            audio_part, sample_rate = self._create_audio_internal(
                phoneme_batch, voice_style, speed
            )
            # Trim silence
            audio_part, _ = trim_audio(audio_part)
            yield audio_part, sample_rate, phoneme_batch

    def close(self) -> None:
        """Clean up resources."""
        if self._voice_db is not None:
            self._voice_db.close()
            self._voice_db = None


# Language code mapping for kokoro-onnx
LANG_CODE_TO_ONNX = {
    "a": "en-us",  # American English
    "b": "en-gb",  # British English
    "e": "es",  # Spanish
    "f": "fr",  # French
    "h": "hi",  # Hindi
    "i": "it",  # Italian
    "j": "ja",  # Japanese
    "p": "pt",  # Portuguese
    "z": "zh",  # Chinese
}


def get_onnx_lang_code(ttsforge_lang: str) -> str:
    """Convert ttsforge language code to kokoro-onnx language code."""
    return LANG_CODE_TO_ONNX.get(ttsforge_lang, "en-us")
